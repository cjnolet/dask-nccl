{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an existing Dask cluster running already, set the scheduler address below. Otherwise, leave it to `None` and a local cluster will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/cjnolet/cuml5/lib/python3.7/site-packages/distributed/bokeh/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46718\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:38116/status' target='_blank'>http://127.0.0.1:38116/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>540.95 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:46718' processes=8 cores=8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler_address = None #\"tcp://10.2.168.161:8786\"\n",
    "\n",
    "if scheduler_address is None:\n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    cluster = LocalCUDACluster()\n",
    "    c = Client(cluster)\n",
    "else:\n",
    "    c = Client(scheduler_address)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nccl_example import nccl, inject_comms_on_handle\n",
    "from cuml.common.handle import Handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import wait\n",
    "from dask.distributed import get_worker\n",
    "\n",
    "import numba.cuda\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import asyncio\n",
    "import ucp\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tornado import gen\n",
    "from dask.distributed import default_client\n",
    "from toolz import first\n",
    "import logging\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "import cudf\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import wait\n",
    "\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "import dask_cudf\n",
    "\n",
    "\n",
    "@gen.coroutine\n",
    "def extract_ddf_partitions(ddf):\n",
    "    \"\"\"\n",
    "    Given a Dask cuDF, return a tuple with (worker, future) for each partition\n",
    "    \"\"\"\n",
    "    client = default_client()\n",
    "    \n",
    "    delayed_ddf = ddf.to_delayed()\n",
    "    parts = client.compute(delayed_ddf)\n",
    "    yield wait(parts)\n",
    "    \n",
    "    key_to_part_dict = dict([(str(part.key), part) for part in parts])\n",
    "    who_has = yield client.who_has(parts)\n",
    "\n",
    "    worker_map = []\n",
    "    for key, workers in who_has.items():\n",
    "        worker = parse_host_port(first(workers))\n",
    "        worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "    gpu_data = [(worker, part) for worker, part in worker_map]\n",
    "\n",
    "    yield wait(gpu_data)\n",
    "\n",
    "    raise gen.Return(gpu_data)\n",
    "    \n",
    "    \n",
    "def create_df(f, m, n, c):\n",
    "    \"\"\"\n",
    "    Generates a cudf of the given size with sklearn's make_blobs \n",
    "    \"\"\"\n",
    "    from sklearn.datasets.samples_generator import make_blobs\n",
    "    X, y = make_blobs(n_samples=m, centers=c, n_features=n, random_state=0)\n",
    "    ret = cudf.DataFrame([(i,\n",
    "                           X[:, i].astype(np.float64)) for i in range(n)],\n",
    "                         index=cudf.dataframe.RangeIndex(f * m,\n",
    "                                                         f * m + m, 1))\n",
    "    return ret\n",
    "\n",
    "def get_meta(df):\n",
    "    ret = df.iloc[:0]\n",
    "    return ret\n",
    "\n",
    "def gen_dask_cudf(nrows, ncols, clusters):\n",
    "    workers = c.has_what().keys()\n",
    "\n",
    "    # Create dfs on each worker (gpu)\n",
    "    dfs = [c.submit(create_df, n, nrows, ncols, clusters, workers=[worker])\n",
    "           for worker, n in list(zip(workers, list(range(len(workers)))))]\n",
    "    # Wait for completion\n",
    "    wait(dfs)\n",
    "\n",
    "    meta = c.submit(get_meta, dfs[0]).result()\n",
    "    return dask_cudf.from_delayed(dfs, meta=meta)\n",
    "\n",
    "def to_dask_cudf(futures):\n",
    "    # Convert a list of futures containing dfs back into a dask_cudf\n",
    "    dfs = [d for d in futures if d.type != type(None)]\n",
    "    meta = c.submit(get_meta, dfs[0]).result()\n",
    "    return dd.from_delayed(dfs, meta=meta)\n",
    "\n",
    "\n",
    "async def connection_func(ep, listener):\n",
    "    print(\"connection received from \" + str(ep))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusable NCCL Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommsBase:\n",
    "    \n",
    "    def __init__(self, comms_coll = True, comms_p2p = False):\n",
    "        self.client = default_client()\n",
    "        self.comms_coll = comms_coll\n",
    "        self.comms_p2p = comms_p2p\n",
    "        \n",
    "        # Used to identify this distinct session on workers\n",
    "        self.sessionId = uuid.uuid4().bytes\n",
    "            \n",
    "        self.worker_addresses = self.get_workers_()\n",
    "        self.workers = list(map(lambda x: parse_host_port(x), self.worker_addresses))\n",
    "\n",
    "    def __dealloc__(self):\n",
    "        self.destroy()\n",
    "\n",
    "    def get_workers_(self, parse_address = True):\n",
    "        \"\"\"\n",
    "        Return the list of workers parsed as [(address, port)]\n",
    "        \"\"\"\n",
    "        return list(self.client.has_what().keys())\n",
    "    \n",
    "    def worker_ranks(self):\n",
    "        \"\"\"\n",
    "        Builds a dictionary of { (worker_address, worker_port) : worker_rank }\n",
    "        \"\"\"\n",
    "        return dict(list(map(lambda x: (x[1], x[0]), self.nccl_clique)))\n",
    "    \n",
    "    def worker_ports(self):\n",
    "        \"\"\"\n",
    "        Builds a dictionary of { (worker_address, worker_port) : worker_port }\n",
    "        \"\"\"\n",
    "        return dict(list(self.ucp_ports))\n",
    "    \n",
    "    \n",
    "    def worker_info(self):\n",
    "        \"\"\"\n",
    "        Builds a dictionary of { (worker_address, worker_port) : (worker_rank, worker_port ) }\n",
    "        \"\"\"\n",
    "        ranks = self.worker_ranks() if self.comms_coll else None\n",
    "        ports = self.worker_ports() if self.comms_p2p else None\n",
    "        \n",
    "        if self.comms_coll and self.comms_p2p:\n",
    "            output = {}\n",
    "            for k in self.worker_ranks().keys():\n",
    "                output[k] = (ranks[k], ports[k])\n",
    "            return output\n",
    "                \n",
    "        elif self.comms_coll:\n",
    "            return ranks\n",
    "        elif self.comms_p2p:\n",
    "            return ports\n",
    "\n",
    "    @staticmethod\n",
    "    def func_init_nccl(workerId, nWorkers, uniqueId):\n",
    "        \"\"\"\n",
    "        Initialize ncclComm_t on worker\n",
    "        \"\"\"\n",
    "        n = nccl()\n",
    "        n.init(nWorkers, uniqueId, workerId)\n",
    "        return n\n",
    "\n",
    "    @staticmethod\n",
    "    def func_get_ucp_port(sessionId, r):\n",
    "        \"\"\"\n",
    "        Return the port assigned to a UCP listener on worker\n",
    "        \"\"\"\n",
    "        dask_worker = get_worker()\n",
    "        port = dask_worker.data[sessionId].port\n",
    "        return port\n",
    "\n",
    "    @staticmethod\n",
    "    async def ucp_create_listener(sessionId, r):\n",
    "        dask_worker = get_worker()\n",
    "        if sessionId in dask_worker.data:\n",
    "            print(\"Listener already started for sessionId=\" + str(sessionId))\n",
    "        else:\n",
    "            ucp.init()\n",
    "            listener =  ucp.start_listener(connection_func, 0, is_coroutine=True)\n",
    "\n",
    "            dask_worker.data[sessionId] = listener\n",
    "            task = asyncio.create_task(listener.coroutine)\n",
    "\n",
    "            while not task.done():\n",
    "                await task\n",
    "                await asyncio.sleep(1)\n",
    "                \n",
    "            ucp.fin()\n",
    "            del dask_worker.data[sessionId]\n",
    "            \n",
    "\n",
    "            \n",
    "    @staticmethod\n",
    "    def ucp_stop_listener(sessionId, r):\n",
    "        dask_worker = get_worker()\n",
    "        if sessionId in dask_worker.data:\n",
    "            listener = dask_worker.data[sessionId]\n",
    "            ucp.stop_listener(listener)\n",
    "        else:\n",
    "            print(\"Listener not found with sessionId=\" + str(sessionId))\n",
    "\n",
    "    def create_ucp_listeners(self):\n",
    "        \"\"\"\n",
    "        Build a UCP listener on each worker. Since this async function is long-running, the listener is \n",
    "        placed in the worker's data dict. \n",
    "        \n",
    "        NOTE: This is not the most ideal design because the worker's data dict could be serialized at \n",
    "        any point, which would cause an error. Need to sync w/ the Dask team to see if there's a better\n",
    "        way to do this. \n",
    "        \"\"\"\n",
    "        [self.client.run(CommsBase.ucp_create_listener, self.sessionId, random.random(), workers = [w], wait = False) for w in \n",
    "         self.worker_addresses]\n",
    "        \n",
    "    def get_ucp_ports(self):\n",
    "        \"\"\"\n",
    "        Return the UCP listener ports attached to this session\n",
    "        \"\"\"\n",
    "        self.ucp_ports = [(w, self.client.submit(CommsBase.func_get_ucp_port, self.sessionId, random.random(), workers = [w]).result()) \n",
    "                          for w in self.workers]\n",
    "        \n",
    "    def stop_ucp_listeners(self):\n",
    "        \"\"\"\n",
    "        Stops the UCP listeners attached to this session\n",
    "        \"\"\"\n",
    "        a = [c.submit(CommsBase.ucp_stop_listener, self.sessionId, random.random(), workers=[w]) \n",
    "         for w in self.workers]\n",
    "        wait(a)\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_build_handle(nccl_comm, eps, nWorkers, workerId):\n",
    "        \n",
    "        ucp_worker = ucp.get_ucp_worker()\n",
    "        \n",
    "        handle = Handle()\n",
    "        inject_comms_on_handle(handle, nccl_comm, ucp_worker, eps, nWorkers, workerId)\n",
    "        return handle\n",
    "\n",
    "    def init_nccl(self):\n",
    "        \"\"\"\n",
    "        Use nccl-py to initialize ncclComm_t on each worker and \n",
    "        store the futures for this instance. \n",
    "        \"\"\"\n",
    "        self.uniqueId = nccl.get_unique_id()\n",
    "\n",
    "        workers_indices = list(zip(self.workers, range(len(self.workers))))\n",
    "\n",
    "        self.nccl_clique = [(idx, worker, self.client.submit(KMeans.func_init_nccl, \n",
    "                                           idx, \n",
    "                                           len(self.workers), \n",
    "                                           self.uniqueId,\n",
    "                                           workers=[worker]))\n",
    "             for worker, idx in workers_indices]\n",
    "        \n",
    "    def init_ucp(self):\n",
    "        \"\"\"\n",
    "        Use ucx-py to initialize ucp endpoints so that every \n",
    "        worker can communicate, point-to-point, with every other worker\n",
    "        \"\"\"\n",
    "        self.create_ucp_listeners()\n",
    "        self.get_ucp_ports()\n",
    "        self.ucp_create_endpoints()\n",
    "        \n",
    "    def init(self):\n",
    "        if self.comms_coll:\n",
    "            self.init_nccl()\n",
    "        \n",
    "        if self.comms_p2p:\n",
    "            self.init_ucp()\n",
    "            \n",
    "        # Combine ucp ports w/ nccl ranks\n",
    "            \n",
    "        eps_futures = dict(self.ucp_endpoints)\n",
    "            \n",
    "        self.handles = [(wid, w, self.client.submit(CommsBase.func_build_handle, f, eps_futures[w], len(self.workers), wid, workers = [w])) \n",
    "                        for wid, w, f in self.nccl_clique]\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    async def func_ucp_create_endpoints(sessionId, worker_info, r):\n",
    "        \"\"\"\n",
    "        Runs on each worker to create ucp endpoints to all other workers\n",
    "        \"\"\"\n",
    "        dask_worker = get_worker()\n",
    "        local_address = parse_host_port(dask_worker.address)\n",
    "        \n",
    "        eps = [None]*len(worker_info)\n",
    "        \n",
    "        count = 1\n",
    "        size = len(worker_info)-1\n",
    "        \n",
    "        for k in worker_info:\n",
    "            if k != local_address:\n",
    "                ip, port = k\n",
    "                rank, ucp_port = worker_info[k]\n",
    "                ep = await ucp.get_endpoint(ip.encode(), ucp_port, timeout = 1)\n",
    "                eps[rank] = ep\n",
    "                count +=1\n",
    "        dask_worker.data[str(sessionId) + \"_eps\"] = eps\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_endpoints(sessionId, r):\n",
    "        \"\"\"\n",
    "        Fetches (and removes) the endpoints from the worker's data dict\n",
    "        \"\"\"\n",
    "        dask_worker = get_worker()\n",
    "        eps = dask_worker.data[str(sessionId)+\"_eps\"]\n",
    "        del dask_worker.data[str(sessionId)+\"_eps\"]\n",
    "        return eps\n",
    "        \n",
    "    def ucp_create_endpoints(self):\n",
    "        \n",
    "        worker_info = self.worker_info()\n",
    "        \n",
    "        [self.client.run(CommsBase.func_ucp_create_endpoints, self.sessionId, worker_info, random.random(), workers = [w], wait = True)\n",
    "                          for w in self.worker_addresses]\n",
    "        \n",
    "        ret = [(w, self.client.submit(CommsBase.func_get_endpoints, self.sessionId, random.random(), workers=[w])) for w in self.workers]\n",
    "        wait(ret)\n",
    "        \n",
    "        self.ucp_endpoints = ret\n",
    "\n",
    "    @staticmethod\n",
    "    def func_destroy_nccl(nccl_comm, r):\n",
    "        \"\"\"\n",
    "        Destroys NCCL communicator on worker\n",
    "        \"\"\"\n",
    "        nccl_comm.destroy()\n",
    "        \n",
    "\n",
    "    def destroy_nccl(self):\n",
    "        \"\"\"\n",
    "        Destroys all NCCL communicators on workers\n",
    "        \"\"\"\n",
    "        a = [self.client.submit(CommsBase.func_destroy_nccl, f, random.random(), workers=[w]) for wid, w, f in self.nccl_clique]\n",
    "        wait(a)\n",
    "        \n",
    "    def func_destroy_ep(eps, r):\n",
    "        \"\"\"\n",
    "        Destroys UCP endpoints on worker\n",
    "        \"\"\"\n",
    "        for ep in eps:\n",
    "            if ep is not None:\n",
    "                ucp.destroy_ep(ep)\n",
    "                \n",
    "    def destroy_eps(self):\n",
    "        \"\"\"\n",
    "        Destroys all UCP endpoints on all workers\n",
    "        \"\"\"\n",
    "        a = [self.client.submit(CommsBase.func_destroy_ep, f, random.random(), workers = [w]) for w, f in self.ucp_endpoints]\n",
    "        wait(a)\n",
    "        \n",
    "    def destroy_ucp(self):\n",
    "        self.destroy_eps()\n",
    "        self.stop_ucp_listeners()\n",
    "        \n",
    "\n",
    "    def destroy(self):\n",
    "\n",
    "        self.handles = None\n",
    "        \n",
    "        if self.comms_p2p:\n",
    "            self.destroy_ucp()\n",
    "            self.ucp_ports = None\n",
    "            self.ucp_endpoints = None\n",
    "\n",
    "        if self.comms_coll:\n",
    "            # TODO: Figure out why this fails when UCP + NCCL are both used\n",
    "#             self.destroy_nccl()\n",
    "            self.nccl_clique = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask-cuML OPG KMeans Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as cumlKMeans\n",
    "\n",
    "class KMeans(CommsBase):\n",
    "    \n",
    "    def __init__(self, n_clusters = 8, init_method = \"random\", verbose = 0):\n",
    "        super(KMeans, self).__init__(comms_coll = True, comms_p2p = True)\n",
    "        self.init_(n_clusters = n_clusters, init_method = init_method, verbose = verbose)\n",
    "\n",
    "    def init_(self, n_clusters, init_method, verbose = 0):\n",
    "        \"\"\"\n",
    "        Creates local kmeans instance on each worker\n",
    "        \"\"\"\n",
    "        self.init()\n",
    "        \n",
    "        self.kmeans = [(w, c.submit(KMeans.func_build_kmeans_, \n",
    "                                    a, n_clusters, init_method, verbose, i, \n",
    "                                    workers=[w])) for i, w, a in self.handles]\n",
    "        wait(self.kmeans)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_build_kmeans_(handle, n_clusters, init_method, verbose, wid):\n",
    "        \"\"\"\n",
    "        Create local KMeans instance on worker\n",
    "        \"\"\"\n",
    "        w = dask.distributed.get_worker()\n",
    "        \n",
    "        return cumlKMeans(handle = handle, init = init_method, n_clusters = n_clusters, verbose = verbose)\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_fit(model, df, wid): return model.fit(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_predict(model, df, wid): return model.predict(df)\n",
    "\n",
    "    def run_model_func_on_dask_cudf(self, func, X):\n",
    "        \n",
    "        gpu_futures = c.sync(extract_ddf_partitions, X)\n",
    "\n",
    "        worker_model_map = dict(map(lambda x: (x[0], x[1]), self.kmeans))\n",
    "        worker_rank_map = self.worker_ranks()\n",
    "\n",
    "        f = [c.submit(func,                              # Function to run on worker\n",
    "                      worker_model_map[w],               # Model instance\n",
    "                      f,                                 # Input DataFrame partition\n",
    "                      worker_rank_map[w])                # Worker ID\n",
    "             for w, f in gpu_futures] \n",
    "        wait(f)\n",
    "        return f\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.run_model_func_on_dask_cudf(KMeans.func_fit, X)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        f = self.run_model_func_on_dask_cudf(KMeans.func_predict, X)\n",
    "        return to_dask_cudf(f)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        return self.fit(X).predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute End-To-End Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_worker = 10\n",
    "n_features = 50\n",
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dask cuDF using sklearn's `make_blobs` for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gen_dask_cudf(n_samples_per_worker, n_features, n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a Dask-cuML `KMeans` instance is created, which initializes it's own NCCL clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = KMeans(n_clusters, init_method = \"random\", verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the ranks assigned to the workers in the NCCL clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('127.0.0.1', 37141): 0,\n",
       " ('127.0.0.1', 39300): 1,\n",
       " ('127.0.0.1', 39305): 2,\n",
       " ('127.0.0.1', 39349): 3,\n",
       " ('127.0.0.1', 39415): 4,\n",
       " ('127.0.0.1', 41404): 5,\n",
       " ('127.0.0.1', 41590): 6,\n",
       " ('127.0.0.1', 41620): 7}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.worker_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify we have one cuDF partition per worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:37141': ('func_get_endpoints-213ba02b18b277caacdafa1607e0648d',\n",
       "  'func_build_kmeans_-10ce05c3ac583aef53e6e8ba78ebdaba',\n",
       "  'create_df-14e81bf95a9b4429e55a40d3db9bdc5e',\n",
       "  'func_build_handle-4d186a1b7aa303b1041b99ee15cdffcd',\n",
       "  'func_init_nccl-43ef591fa7f9de497cd33568b50cec93'),\n",
       " 'tcp://127.0.0.1:39300': ('func_init_nccl-d10a6c3596b064632bd5d9fee14e7332',\n",
       "  'func_build_handle-be5f29e285d4976eaa5d60b6ffdeb785',\n",
       "  'create_df-01846fbe2c680b2ec06cddce6288ace1',\n",
       "  'func_get_endpoints-77e49dc6ba2c0123a9714787c6721204',\n",
       "  'func_build_kmeans_-4fbf7cff2b285a3d049efdf782abde5c'),\n",
       " 'tcp://127.0.0.1:39305': ('create_df-db0b1ab4a3df18d1c2752b47a9e05a02',\n",
       "  'func_get_endpoints-f345f42380219bbe28d5f8880b3fb031',\n",
       "  'func_build_kmeans_-fed32eed3cf763ca6dbddf421acccdc2',\n",
       "  'func_init_nccl-c4fc3fb5343136e13d8043a8641b4a35',\n",
       "  'func_build_handle-96ff7342a8e8a59f8418d72745712d61'),\n",
       " 'tcp://127.0.0.1:39349': ('func_init_nccl-c374cbfc684464757de8aceb48a1162e',\n",
       "  'func_build_handle-6e73faaa3775ebe4af0acbc4dd53782d',\n",
       "  'func_build_kmeans_-1b42676b4db8f10368fe1badb0464caf',\n",
       "  'func_get_endpoints-6c636bfd03b4f15ec20ca1b35de199c4',\n",
       "  'create_df-200d64432fe3a398aee4de1ef551bf31'),\n",
       " 'tcp://127.0.0.1:39415': ('create_df-e7130f2974d60ab18f281c4ad3a8f1bd',\n",
       "  'func_build_handle-0f34f80f21e563a8a542d34cdc3c336e',\n",
       "  'func_init_nccl-310c5718c62c55ca8829ae8200b9a8c8',\n",
       "  'func_build_kmeans_-0b0d0b0eeded9012a9f40336892a1e55',\n",
       "  'func_get_endpoints-eafa783118f5b19544b7d83898ea0b80'),\n",
       " 'tcp://127.0.0.1:41404': ('func_init_nccl-91204484bafdc50dbef75c9918e83323',\n",
       "  'func_build_handle-ff24e2eb3b54883f9d6bce34be4f214d',\n",
       "  'func_get_endpoints-8d0e0a4314cd549935220cc0900fc63c',\n",
       "  'create_df-87f2d380fd6a4458c7b1a5e3b80f5b55',\n",
       "  'func_build_kmeans_-220d033a14cbd37bf066ab5b3160380a'),\n",
       " 'tcp://127.0.0.1:41590': ('create_df-4815200848ecdb51469278fee89c9318',\n",
       "  'func_build_handle-bc53672c51d9b2ee69f78c79e5e96ed4',\n",
       "  'func_init_nccl-bf3960d9f27046b3d7455ff255e9f022',\n",
       "  'func_build_kmeans_-512fee071e38ac2dd2e1b9c65e6413f5',\n",
       "  'func_get_endpoints-21231cf782a2e5639443bad65ce3aee5'),\n",
       " 'tcp://127.0.0.1:41620': ('create_df-c0c3b7afdaa4450219bd2f86dc5016d4',\n",
       "  'func_get_endpoints-6c4e9eb32e0293555d5719f95bfea263',\n",
       "  'func_build_kmeans_-f59f27284fdab76b0d5f625a817188d2',\n",
       "  'func_init_nccl-6244b6dc125aeec8869e9181cb60930d',\n",
       "  'func_build_handle-b8671c59f82c8502b0a5620fdf12365c')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.has_what()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('127.0.0.1', 37141): 0,\n",
       " ('127.0.0.1', 39300): 1,\n",
       " ('127.0.0.1', 39305): 2,\n",
       " ('127.0.0.1', 39349): 3,\n",
       " ('127.0.0.1', 39415): 4,\n",
       " ('127.0.0.1', 41404): 5,\n",
       " ('127.0.0.1', 41590): 6,\n",
       " ('127.0.0.1', 41620): 7}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.worker_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the KMeans MNMG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KMeans at 0x7ff9c03d5ba8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels for the same inputs we trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = demo.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dask_cudf.Series | 16 tasks | 8 npartitions>\n"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "5    2\n",
      "6    3\n",
      "7    2\n",
      "8    0\n",
      "9    1\n",
      "[70 more rows]\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(str(result.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuml5 [conda env:cuml5]",
   "language": "python",
   "name": "conda-env-cuml5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
