{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an existing Dask cluster running already, set the scheduler address below. Otherwise, leave it to `None` and a local cluster will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/cjnolet/cuml5/lib/python3.7/site-packages/distributed/bokeh/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37488\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:36592/status' target='_blank'>http://127.0.0.1:36592/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>540.95 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:37488' processes=8 cores=8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler_address = None #\"tcp://10.2.168.161:8786\"\n",
    "\n",
    "if scheduler_address is None:\n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    cluster = LocalCUDACluster()\n",
    "    c = Client(cluster)\n",
    "else:\n",
    "    c = Client(scheduler_address)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nccl_example import nccl, inject_comms_on_handle, SimpleReduce\n",
    "from cuml.common.handle import Handle\n",
    "\n",
    "import random\n",
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "import numba.cuda\n",
    "import cudf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tornado import gen\n",
    "from dask.distributed import default_client\n",
    "from toolz import first\n",
    "import logging\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "import cudf\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import wait\n",
    "\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "import dask_cudf\n",
    "\n",
    "\n",
    "@gen.coroutine\n",
    "def extract_ddf_partitions(ddf):\n",
    "    \"\"\"\n",
    "    Given a Dask cuDF, return a tuple with (worker, future) for each partition\n",
    "    \"\"\"\n",
    "    client = default_client()\n",
    "    \n",
    "    delayed_ddf = ddf.to_delayed()\n",
    "    parts = client.compute(delayed_ddf)\n",
    "    yield wait(parts)\n",
    "    \n",
    "    key_to_part_dict = dict([(str(part.key), part) for part in parts])\n",
    "    who_has = yield client.who_has(parts)\n",
    "\n",
    "    worker_map = []\n",
    "    for key, workers in who_has.items():\n",
    "        worker = parse_host_port(first(workers))\n",
    "        worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "    gpu_data = [(worker, part) for worker, part in worker_map]\n",
    "\n",
    "    yield wait(gpu_data)\n",
    "\n",
    "    raise gen.Return(gpu_data)\n",
    "    \n",
    "    \n",
    "def create_df(f, m, n, c):\n",
    "    \"\"\"\n",
    "    Generates a cudf of the given size with all values initialized to 1 \n",
    "    \"\"\"\n",
    "    from sklearn.datasets.samples_generator import make_blobs\n",
    "    X, y = make_blobs(n_samples=m, centers=c, n_features=n, random_state=0)\n",
    "    ret = cudf.DataFrame([(i,\n",
    "                           X[:, i].astype(np.float64)) for i in range(n)],\n",
    "                         index=cudf.dataframe.RangeIndex(f * m,\n",
    "                                                         f * m + m, 1))\n",
    "    return ret\n",
    "\n",
    "def get_meta(df):\n",
    "    ret = df.iloc[:0]\n",
    "    return ret\n",
    "\n",
    "def gen_dask_cudf(nrows, ncols, clusters):\n",
    "    workers = c.has_what().keys()\n",
    "\n",
    "    # Create dfs on each worker (gpu)\n",
    "    dfs = [c.submit(create_df, n, nrows, ncols, clusters, workers=[worker])\n",
    "           for worker, n in list(zip(workers, list(range(len(workers)))))]\n",
    "    # Wait for completion\n",
    "    wait(dfs)\n",
    "\n",
    "    meta = c.submit(get_meta, dfs[0]).result()\n",
    "    return dask_cudf.from_delayed(dfs, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans as cumlKMeans\n",
    "\n",
    "class KMeans:\n",
    "    \n",
    "    def __init__(self, n_clusters = 8, init_method = \"random\", verbose = 0):\n",
    "        self.client = default_client()\n",
    "\n",
    "        self.init_(n_clusters = n_clusters, init_method = init_method, verbose = verbose)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def func_init_(workerId, nWorkers, uniqueId, n_clusters, init, verbose = 0):\n",
    "        \"\"\"\n",
    "        Initialize ncclComm_t on worker\n",
    "        \"\"\"\n",
    "        w = dask.distributed.get_worker()\n",
    "\n",
    "        # 1. initialize any necessary comms on each worker\n",
    "        n = nccl()\n",
    "        n.init(nWorkers, uniqueId, workerId)\n",
    "\n",
    "        # 2. Initialize cumlCommunicator and inject into cumlHandle\n",
    "        handle = Handle()\n",
    "        inject_comms_on_handle(handle, n, nWorkers, workerId)\n",
    "\n",
    "        # 3. Use the cumlHandle w/ cumlCommunicator for model\n",
    "        a = cumlKMeans(handle = handle, init = init, n_clusters = n_clusters, verbose = verbose)\n",
    "        #a = SimpleReduce(workerId, nWorkers, n.get_comm())\n",
    "\n",
    "        return (n, a)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_meta(df):\n",
    "        return df.iloc[:0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_fit(model, df, r):\n",
    "        \"\"\"\n",
    "        This function is executed on the workers and performs the necessary\n",
    "        data preparation, as well as calling the cython-wrapped C++ \"algorithm\"\n",
    "        function(s), returning a cuDF with results, if necessary. \n",
    "        \n",
    "        The client will construct a Dask cuDF out of the cuDFs returned from\n",
    "        this function, if necessary. \n",
    "        \"\"\"\n",
    "\n",
    "        nccl_comm, model = model\n",
    "        \n",
    "        # Execute our cython-wrapped C++ \"algorithm\"\n",
    "        return model.fit(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_predict(model, df, r):\n",
    "        \"\"\"\n",
    "        This function is executed on the workers and performs the necessary\n",
    "        data preparation, as well as calling the cython-wrapped C++ \"algorithm\"\n",
    "        function(s), returning a cuDF with results, if necessary. \n",
    "        \n",
    "        The client will construct a Dask cuDF out of the cuDFs returned from\n",
    "        this function, if necessary. \n",
    "        \"\"\"\n",
    "        nccl_comm, model = model\n",
    "        \n",
    "        # Execute our cython-wrapped C++ \"algorithm\"\n",
    "        return model.predict(df)\n",
    "\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        An end to end example to mimic a typical Dask cuML algorithm using\n",
    "        OPG semantics. This function is executed on the client.\n",
    "        \n",
    "        The following steps are taken with Dask cuDF as input:\n",
    "        1. Co-locate Dask cuDF partitions with nccl communicator and our demo \"model\" on each worker\n",
    "        2. Run the algorithm, extracting the Numba device memory pointer for each partition\n",
    "           and allocating necessary output memory on device for constructing the cuDF \n",
    "           partition(s) that will be returned to the user. \n",
    "        3. Construct Dask cuDF from the futures containing the cuDFs returned from local \n",
    "           \"algorithm\" functions on each worker. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures = c.sync(extract_ddf_partitions, X)\n",
    "\n",
    "        worker_model_map = dict(map(lambda x: (x[1], x[2]), self.clique))\n",
    "\n",
    "        # Run our \"algorithm\" to perform reduce \n",
    "        f = [c.submit(KMeans.func_fit, # Function to run on worker\n",
    "                      worker_model_map[w],               # tuple(nccl_comm, KMeans instance)\n",
    "                      f,                                 # Input DataFrame partition\n",
    "                      random.random())                   # Makes sure all workers call function \n",
    "             for w, f in gpu_futures] \n",
    "        wait(f)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures = c.sync(extract_ddf_partitions, X)\n",
    "\n",
    "        worker_model_map = dict(map(lambda x: (x[1], x[2]), self.clique))\n",
    "\n",
    "        # Run our \"algorithm\" to perform reduce \n",
    "        f = [c.submit(KMeans.func_predict, # Function to run on worker\n",
    "                      worker_model_map[w],               # tuple(nccl_comm, KMeans instance)\n",
    "                      f,                                 # Input DataFrame partition\n",
    "                      random.random())                   # Makes sure all workers call function \n",
    "             for w, f in gpu_futures] \n",
    "        wait(f)\n",
    "        \n",
    "        # Convert result back into a dask_cudf\n",
    "        dfs = [d for d in f if d.type != type(None)]\n",
    "        meta = c.submit(KMeans.get_meta, dfs[0]).result()\n",
    "        ddf = dd.from_delayed(dfs, meta=meta)\n",
    "        \n",
    "        return ddf\n",
    "\n",
    "    def worker_ranks(self):\n",
    "        \"\"\"\n",
    "        Builds a dictionary of { (worker_address, worker_port) : worker_rank }\n",
    "        \"\"\"\n",
    "        return dict(list(map(lambda x: (x[1], x[0]), self.clique)))\n",
    "\n",
    "    def run_func_on_workers(self, func):\n",
    "        \"\"\"\n",
    "        Simple helper function to schedule a function on all workers\n",
    "        of a clique\n",
    "        \"\"\"\n",
    "        f = [c.submit(func, a, random.random()) for i, w, a in self.clique]\n",
    "        wait(f)\n",
    "        return [a.result() for a in f]\n",
    "\n",
    "    def get_workers_(self):\n",
    "        \"\"\"\n",
    "        Return the list of workers parsed as [(address, port)]\n",
    "        \"\"\"\n",
    "        return list(map(lambda x: parse_host_port(x), self.client.has_what().keys()))\n",
    "    \n",
    "    def init_(self, n_clusters, init_method, verbose = 0):\n",
    "        \"\"\"\n",
    "        Use nccl-py to initialize ncclComm_t on each worker and \n",
    "        store the futures for this instance. \n",
    "        \"\"\"\n",
    "        uniqueId = nccl.get_unique_id()\n",
    "\n",
    "        workers = self.get_workers_()\n",
    "        workers_indices = list(zip(workers, range(len(workers))))\n",
    "\n",
    "        self.clique = [(idx, worker, self.client.submit(KMeans.func_init_, \n",
    "                                           idx, \n",
    "                                           len(workers), \n",
    "                                           uniqueId,\n",
    "                                           n_clusters,\n",
    "                                           init_method,\n",
    "                                           verbose,\n",
    "                                           workers=[worker]))\n",
    "             for worker, idx in workers_indices]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a Dask-cuML `KMeans` instance is created, which initializes it's own NCCL clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dask_NCCL_Demo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b01373550bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"random\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-edc62209599a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_clusters, init_method, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-edc62209599a>\u001b[0m in \u001b[0;36minit_\u001b[0;34m(self, n_clusters, init_method, verbose)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                            \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                                            workers=[worker]))\n\u001b[0;32m--> 160\u001b[0;31m              for worker, idx in workers_indices]\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-edc62209599a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                            \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                                            workers=[worker]))\n\u001b[0;32m--> 160\u001b[0;31m              for worker, idx in workers_indices]\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dask_NCCL_Demo' is not defined"
     ]
    }
   ],
   "source": [
    "demo = KMeans(n_clusters, init_method = \"random\", verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the ranks assigned to the workers in the NCCL clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.worker_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crate a Dask cuDF using sklearn's `make_blobs` for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gen_dask_cudf(10, 50, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate we have one cuDF partition per worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:33214': ('func_predict-71015d670d5c778aa1c954b4d6ce123e',\n",
       "  'create_df-c6a8bfc80639c2fbdb9cbd02de40084a'),\n",
       " 'tcp://127.0.0.1:39329': ('func_predict-ad7748343b8119d547580ba4b129c22d',\n",
       "  'create_df-b5596daa0e9d4e1b90ebd3350d7c694a'),\n",
       " 'tcp://127.0.0.1:39363': ('func_predict-ea3166be694179ae83ebad35561e40e1',\n",
       "  'create_df-b5d2222a97e4f5d2fe6366a99349ebb3'),\n",
       " 'tcp://127.0.0.1:39822': ('create_df-de921811da61b39207148eaa08a3679a',\n",
       "  'func_predict-5410147a2215d9d513bfc0a57ab180fd'),\n",
       " 'tcp://127.0.0.1:40041': ('func_predict-dbc507e01bb580ea8f49a4fe532fce6e',\n",
       "  'create_df-d1136d784920bf0bf7b42b8eee43c2bf'),\n",
       " 'tcp://127.0.0.1:41081': ('create_df-960aa22cb1b56865a10f6f2a1ca47eda',\n",
       "  'func_predict-6e49a2d1dd9eb2654c655e22a4c391e0'),\n",
       " 'tcp://127.0.0.1:45709': ('create_df-e0c3cedc252d8426008c0808533a7dc3',\n",
       "  'func_predict-de99e03ddeb6e459f60f8d2fca81140f'),\n",
       " 'tcp://127.0.0.1:46718': ('create_df-ef885c3a54df7aa085194e450fe7df64',\n",
       "  'func_predict-2dbe4e05edb0ea47c16d5dcb250edb34')}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.has_what()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the KMeans MNMG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels for the same inputs we trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected bytes, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-96f344deabf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-a401d7f6bb4f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Convert result back into a dask_cudf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDask_NCCL_Demo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_delayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/cjnolet/cuml5/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraiseit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cancelled\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/cjnolet/cuml5/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-77e0b824f4de>\u001b[0m in \u001b[0;36mfunc_init_\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# 1. initialize any necessary comms on each worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnccl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnWorkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniqueId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkerId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 2. Initialize cumlCommunicator and inject into cumlHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/cjnolet/workspace/dask-nccl/nccl-example/simple_reduce.pyx\u001b[0m in \u001b[0;36mnccl_example.nccl.init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnranks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mcdef\u001b[0m \u001b[0mncclUniqueId\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mident\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mncclUniqueId\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mmalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncclUniqueId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mncclUniqueIdFromChar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected bytes, int found"
     ]
    }
   ],
   "source": [
    "result = demo.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dask_cudf.Series | 16 tasks | 8 npartitions>\n"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "5    2\n",
      "6    5\n",
      "7    2\n",
      "8    0\n",
      "9    1\n",
      "[70 more rows]\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(str(result.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuml5 [conda env:cuml5]",
   "language": "python",
   "name": "conda-env-cuml5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
