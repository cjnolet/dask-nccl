{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an existing Dask cluster running already, set the scheduler address below. Otherwise, leave it to `None` and a local cluster will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:32868\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>50.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:32868' processes=2 cores=2>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler_address = None #\"tcp://10.2.168.161:8786\"\n",
    "\n",
    "if scheduler_address is None:\n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    cluster = LocalCUDACluster()\n",
    "    c = Client(cluster)\n",
    "else:\n",
    "    c = Client(scheduler_address)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nccl_example import nccl, SimpleReduce\n",
    "import random\n",
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "import numba.cuda\n",
    "import cudf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tornado import gen\n",
    "from dask.distributed import default_client\n",
    "from toolz import first\n",
    "import logging\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "import cudf\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import wait\n",
    "\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "\n",
    "def build_host_dict(workers):\n",
    "    hosts = set(map(lambda x: parse_host_port(x), workers))\n",
    "    hosts_dict = {}\n",
    "    for host, port in hosts:\n",
    "        if host not in hosts_dict:\n",
    "            hosts_dict[host] = set([port])\n",
    "        else:\n",
    "            hosts_dict[host].add(port)\n",
    "\n",
    "    return hosts_dict\n",
    "\n",
    "def _build_host_dict(gpu_futures, client):\n",
    "    \"\"\"\n",
    "    Helper function to build a dictionary mapping workers to parts\n",
    "    that currently hold the parts of given futures.\n",
    "    :param gpu_futures:\n",
    "    :param client:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    who_has = client.who_has(gpu_futures)\n",
    "\n",
    "    key_to_host_dict = {}\n",
    "    for key in who_has:\n",
    "        key_to_host_dict[key] = parse_host_port(who_has[key][0])\n",
    "\n",
    "    hosts_to_key_dict = {}\n",
    "    for key, host in key_to_host_dict.items():\n",
    "        if host not in hosts_to_key_dict:\n",
    "            hosts_to_key_dict[host] = set([key])\n",
    "        else:\n",
    "            hosts_to_key_dict[host].add(key)\n",
    "\n",
    "    workers = [key[0] for key in list(who_has.values())]\n",
    "    return build_host_dict(workers)\n",
    "\n",
    "def to_gpu_matrix(i):\n",
    "    return i\n",
    "\n",
    "@gen.coroutine\n",
    "def _get_mg_info(ddf):\n",
    "    \"\"\"\n",
    "    Given a Dask cuDF, extract number of dimensions and convert\n",
    "    the pieces of the Dask cuDF into Numba arrays, which can\n",
    "    be passed into the kNN algorithm.\n",
    "    build a\n",
    "    :param ddf:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    client = default_client()\n",
    "\n",
    "    if isinstance(ddf, dd.DataFrame):\n",
    "        cols = len(ddf.columns)\n",
    "        parts = ddf.to_delayed()\n",
    "        parts = client.compute(parts)\n",
    "        yield wait(parts)\n",
    "    else:\n",
    "        raise Exception(\"Input should be a Dask DataFrame\")\n",
    "\n",
    "    key_to_part_dict = dict([(str(part.key), part) for part in parts])\n",
    "    who_has = yield client.who_has(parts)\n",
    "\n",
    "    worker_map = []\n",
    "    for key, workers in who_has.items():\n",
    "        worker = parse_host_port(first(workers))\n",
    "        worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "    gpu_data = [(worker, client.submit(to_gpu_matrix, part,\n",
    "                                       workers=[worker]))\n",
    "                for worker, part in worker_map]\n",
    "\n",
    "    yield wait(gpu_data)\n",
    "\n",
    "    raise gen.Return((gpu_data, cols))\n",
    "    \n",
    "import dask_cudf\n",
    "    \n",
    "def create_df(f, m, n):\n",
    "    \n",
    "    print(\"In function\")\n",
    "    X = np.ones((m, n), dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    print(\"Creatin df\")\n",
    "    ret = cudf.DataFrame([(i,\n",
    "                           X[:, i].astype(np.float32)) for i in range(n)],\n",
    "                         index=cudf.dataframe.RangeIndex(f * m,\n",
    "                                                         f * m + m, 1))\n",
    "    \n",
    "    print(\"done\")\n",
    "    return ret\n",
    "\n",
    "def get_meta(df):\n",
    "    ret = df.iloc[:0]\n",
    "    return ret\n",
    "\n",
    "def gen_dask_cudf(nrows, ncols):\n",
    "    workers = c.has_what().keys()\n",
    "\n",
    "    # Create dfs on each worker (gpu)\n",
    "    dfs = [c.submit(create_df, n, nrows, ncols, workers=[worker])\n",
    "           for worker, n in list(zip(workers, list(range(len(workers)))))]\n",
    "    # Wait for completion\n",
    "    wait(dfs)\n",
    "\n",
    "    meta = c.submit(get_meta, dfs[0]).result()\n",
    "    return dask_cudf.from_delayed(dfs, meta=meta)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dask_NCCL_Demo:\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_parse_host_port_(address):\n",
    "        if '://' in address:\n",
    "            address = address.rsplit('://', 1)[1]\n",
    "        host, port = address.split(':')\n",
    "        port = int(port)\n",
    "        return host, port\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_init_(workerId, nWorkers, uniqueId):\n",
    "        w = dask.distributed.get_worker()\n",
    "\n",
    "        n = nccl()\n",
    "        n.init(nWorkers, uniqueId, workerId)\n",
    "        \n",
    "        print(str(\"Rank in python: \" + str(n.user_rank())))\n",
    "\n",
    "        a = SimpleReduce(workerId, nWorkers, n.get_comm())\n",
    "\n",
    "        return (n, a)\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_size_(cuml_comm, r):\n",
    "        return cuml_comm[1].get_clique_size()\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_rank_(cuml_comm, r):\n",
    "        return cuml_comm[1].get_rank()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_meta(df):\n",
    "        return df.iloc[:0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def p(i, r):\n",
    "        print(\"COMB: \" + str(i))\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_test_reduce_on_partition(model, df, root_rank, r):\n",
    "        \"\"\"\n",
    "        This function is executed on the workers and performs the necessary\n",
    "        data preparation, as well as calling the cython-wrapped C++ \"algorithm\"\n",
    "        function(s), returning a cuDF with results, if necessary. \n",
    "        \n",
    "        The client will construct a Dask cuDF out of the cuDFs returned from\n",
    "        this function, if necessary. \n",
    "        \"\"\"\n",
    "\n",
    "        nccl_comm, model = model\n",
    "\n",
    "        # Execute our cython-wrapped C++ \"algorithm\"\n",
    "        return model.fit(df).transform(df)\n",
    "\n",
    "    def test_end_to_end(self, X):\n",
    "        \"\"\"\n",
    "        An end to end example to mimic a typical Dask cuML algorithm using\n",
    "        OPG semantics. This function is executed on the client.\n",
    "        \n",
    "        The following steps are taken with Dask cuDF as input:\n",
    "        1. Co-locate Dask cuDF partitions with nccl communicator and our demo \"model\" on each worker\n",
    "        2. Run the algorithm, extracting the Numba device memory pointer for each partition\n",
    "           and allocating necessary output memory on device for constructing the cuDF \n",
    "           partition(s) that will be returned to the user. \n",
    "        3. Construct Dask cuDF from the futures containing the cuDFs returned from local \n",
    "           \"algorithm\" functions on each worker. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures, cols = c.sync(_get_mg_info, X)\n",
    "\n",
    "        worker_model_map = dict(map(lambda x: (x[1], x[2]), self.clique))\n",
    "\n",
    "        # Run our \"algorithm\" to perform reduce \n",
    "        f = [c.submit(Dask_NCCL_Demo.func_test_reduce_on_partition,   # Function to run on worker\n",
    "                      worker_model_map[w],                            # tuple(nccl_comm, SimpleReduce instance)\n",
    "                      f,                                              # Input DataFrame partition\n",
    "                      0,                                              # Root rank\n",
    "                      random.random())                                # Makes sure all workers call function \n",
    "             for w, f in gpu_futures] \n",
    "        wait(f)\n",
    "        \n",
    "        # Convert result back into a dask_cudf\n",
    "        dfs = [d for d in f if d.type != type(None)]\n",
    "        meta = c.submit(Dask_NCCL_Demo.get_meta, dfs[0]).result()\n",
    "        ddf = dd.from_delayed(dfs, meta=meta)\n",
    "        \n",
    "        return ddf\n",
    "    \n",
    "    def worker_ranks(self):\n",
    "        return dict(list(map(lambda x: (x[1], x[0]), self.clique)))\n",
    "    \n",
    "    \n",
    "    def run_func_on_workers(self, func):\n",
    "        f = [c.submit(func, a, random.random()) for i, w, a in self.clique]\n",
    "        wait(f)\n",
    "        return [a.result() for a in f]\n",
    "\n",
    "    def get_workers_(self):\n",
    "        return list(map(lambda x: Dask_NCCL_Demo.func_parse_host_port_(x), self.client.has_what().keys()))\n",
    "    \n",
    "    def init(self, uniqueId):\n",
    "        workers = self.get_workers_()\n",
    "        workers_indices = list(zip(workers, range(len(workers))))\n",
    "\n",
    "        self.clique = [(idx, worker, self.client.submit(Dask_NCCL_Demo.func_init_, \n",
    "                                           idx, \n",
    "                                           len(workers), \n",
    "                                           uniqueId,\n",
    "                                           workers=[worker]))\n",
    "             for worker, idx in workers_indices]\n",
    "        \n",
    "    def get_clique_size(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_get_size_)\n",
    "\n",
    "    def get_rank(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_get_rank_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First demonstration will show that NCCL works within Dask as expected. This includes:\n",
    "1. Creation of a NCCL clique using Dask workers to broadcast the ncclUniqueId\n",
    "2. Demonstrating the NCCL clique successfully performs collective comms\n",
    "3. All calls to the underlying NCCL comm are made through the cuML comms facade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_id = nccl.get_unique_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = Dask_NCCL_Demo(c)\n",
    "demo.init(world_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.get_clique_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('127.0.0.1', 37041): 0, ('127.0.0.1', 44573): 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.worker_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second demonstration will show that we can take a Dask cuDF and utilize NCCL on the workers hosting its partitions. This includes:\n",
    "1. Initializing a NCCL clique with the workers that host partitions underlying the Dask cuDF\n",
    "2. Perform a collective comm operation across the partitions using only the cuML comms facade to communicate with the NCCL comm. \n",
    "3. Output a Dask cuDF with the results of the collective comm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gen_dask_cudf(10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:37041': ('to_gpu_matrix-851641cbc7bb2b5b4db01b97fcb8d769',\n",
       "  'func_test_reduce_on_partition-bec82cdcb48091b38d8337fa710a697c',\n",
       "  'func_test_reduce_on_partition-2a516ed4c9fe44c2b98c200e3726b0a8',\n",
       "  'func_init_-ceac62dc5d047e2f721546d32dfdc686',\n",
       "  'func_init_-84851ab71fca423f4cdefe87fd80b3d3',\n",
       "  'func_init_-1c99f05f804e70c6b914998281f1a908',\n",
       "  'tuple-013428e2-ab5d-461c-ad4f-8c02703f3529',\n",
       "  'to_gpu_matrix-1ba5ed9c9df9dee3b5cccbfa8fe189e2',\n",
       "  'create_df-960fca4f866c7b77c67fa83b5fa8677d'),\n",
       " 'tcp://127.0.0.1:44573': ('func_init_-607d7c71e1c79ce88282d431de9cc03e',\n",
       "  'tuple-23c576e9-5344-4775-bfe6-0a49537f054a',\n",
       "  'to_gpu_matrix-f47abc700e55a28425d7ba173f25c3d6',\n",
       "  'create_df-acc8aae69a399d8bf0879ec2adb24d47',\n",
       "  'to_gpu_matrix-2f249019813a70c3affcaa21b20feca1',\n",
       "  'func_init_-6d8717e9e9fac8fa6426b33237290852',\n",
       "  'func_test_reduce_on_partition-09d7ac80bf26ac942880c75e320845f2',\n",
       "  'func_init_-537a55f4d0df73878b1b1619d41337c5')}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.has_what()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = demo.test_end_to_end(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dask_cudf.DataFrame | 2 tasks | 1 npartitions>\n"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6 ...   49\n",
      "0  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "1  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "2  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "3  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "4  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "5  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "6  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "7  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "8  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "9  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "[42 more columns]\n"
     ]
    }
   ],
   "source": [
    "print(str(result.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuml4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
