{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import subprocess as subp\n",
    "import os\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37140\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>50.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:37140' processes=2 cores=2>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(cluster)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First demonstration will show that NCCL works within Dask as expected. This includes:\n",
    "1. Creation of a NCCL clique using Dask workers to broadcast the ncclUniqueId\n",
    "2. Demonstrating the NCCL clique successfully performs collective comms\n",
    "3. All calls to the underlying NCCL comm are made through the cuML comms facade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nccl_example import NCCL_Clique, unique_id\n",
    "import random\n",
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second demonstration will show that we can take a Dask cuDF and utilize NCCL on the workers hosting its partitions. This includes:\n",
    "1. Initializing a NCCL clique with the workers that host partitions underlying the Dask cuDF\n",
    "2. Perform a collective comm operation across the partitions using only the cuML comms facade to communicate with the NCCL comm. \n",
    "3. Output a Dask cuDF with the results of the collective comm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "import numpy as np\n",
    "import cudf\n",
    "import pandas as pd\n",
    "from tornado import gen\n",
    "from dask.distributed import default_client\n",
    "import dask.dataframe as dd\n",
    "from toolz import first\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "\n",
    "def build_host_dict(workers):\n",
    "    hosts = set(map(lambda x: parse_host_port(x), workers))\n",
    "    hosts_dict = {}\n",
    "    for host, port in hosts:\n",
    "        if host not in hosts_dict:\n",
    "            hosts_dict[host] = set([port])\n",
    "        else:\n",
    "            hosts_dict[host].add(port)\n",
    "\n",
    "    return hosts_dict\n",
    "\n",
    "def _build_host_dict(gpu_futures, client):\n",
    "    \"\"\"\n",
    "    Helper function to build a dictionary mapping workers to parts\n",
    "    that currently hold the parts of given futures.\n",
    "    :param gpu_futures:\n",
    "    :param client:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    who_has = client.who_has(gpu_futures)\n",
    "\n",
    "    key_to_host_dict = {}\n",
    "    for key in who_has:\n",
    "        key_to_host_dict[key] = parse_host_port(who_has[key][0])\n",
    "\n",
    "    hosts_to_key_dict = {}\n",
    "    for key, host in key_to_host_dict.items():\n",
    "        if host not in hosts_to_key_dict:\n",
    "            hosts_to_key_dict[host] = set([key])\n",
    "        else:\n",
    "            hosts_to_key_dict[host].add(key)\n",
    "\n",
    "    workers = [key[0] for key in list(who_has.values())]\n",
    "    return build_host_dict(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dask_cudf(m, n, demo):\n",
    "    \"\"\"Generates a large Dask cuDF initialized to all 1's (for verifying reduction op)\"\"\"\n",
    "    n_workers = demo.get_clique_size()[0]\n",
    "    X = np.ones((m*n_workers, n), dtype = np.float32)\n",
    "    X = cudf.DataFrame.from_pandas(pd.DataFrame({'fea%d' % i: X[0:, i] for i in range(X.shape[1])}))\n",
    "    return dask_cudf.from_cudf(X, npartitions = n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gpu_matrix(i):\n",
    "    return i\n",
    "\n",
    "@gen.coroutine\n",
    "def _get_mg_info(ddf):\n",
    "    \"\"\"\n",
    "    Given a Dask cuDF, extract number of dimensions and convert\n",
    "    the pieces of the Dask cuDF into Numba arrays, which can\n",
    "    be passed into the kNN algorithm.\n",
    "    build a\n",
    "    :param ddf:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    client = default_client()\n",
    "\n",
    "    if isinstance(ddf, dd.DataFrame):\n",
    "        cols = len(ddf.columns)\n",
    "        parts = ddf.to_delayed()\n",
    "        parts = client.compute(parts)\n",
    "        yield wait(parts)\n",
    "    else:\n",
    "        raise Exception(\"Input should be a Dask DataFrame\")\n",
    "\n",
    "    key_to_part_dict = dict([(str(part.key), part) for part in parts])\n",
    "    who_has = yield client.who_has(parts)\n",
    "\n",
    "    worker_map = []\n",
    "    for key, workers in who_has.items():\n",
    "        worker = parse_host_port(first(workers))\n",
    "        worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "    gpu_data = [(worker, client.submit(to_gpu_matrix, part,\n",
    "                                       workers=[worker]))\n",
    "                for worker, part in worker_map]\n",
    "\n",
    "    yield wait(gpu_data)\n",
    "\n",
    "    raise gen.Return((gpu_data, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_worker(rank, gpu_matrix):\n",
    "    \"\"\"\n",
    "    Function to run on cudf partitions which:\n",
    "    1. extracts the underlying ctypes pointer, \n",
    "    2. calls reduce on it in C++, and\n",
    "    3. provides a single result array on the root worker (lowest rank) that\n",
    "       can be used to construct an output cudf. \n",
    "    \"\"\"\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dask_NCCL_Demo:\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_parse_host_port_(address):\n",
    "        if '://' in address:\n",
    "            address = address.rsplit('://', 1)[1]\n",
    "        host, port = address.split(':')\n",
    "        port = int(port)\n",
    "        return host, port\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_init_(workerId, nWorkers, uniqueId):\n",
    "        w = dask.distributed.get_worker()\n",
    "        \n",
    "        print(\"UNIQUEID: \"  + str(uniqueId))\n",
    "        \n",
    "        print(\"Hello World! from ip=%s worker=%s/%d\" % \\\n",
    "              (w.address, w.name, nWorkers))\n",
    "        a = NCCL_Clique(workerId, nWorkers)\n",
    "        a.create_clique(uniqueId)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_size_(world, r):\n",
    "        return world.get_clique_size()\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_rank_(world, r):\n",
    "        return world.get_rank()\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_device_(world, r):\n",
    "        return world.get_device()\n",
    "\n",
    "    @staticmethod\n",
    "    def func_test_all_reduce_(world, r):\n",
    "        return world.test_all_reduce()\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_test_reduce_on_partition(world_df, root_rank, r):\n",
    "        world, df = world_df\n",
    "        world.test_on_partition(df, root_rank)\n",
    "        \n",
    "        \n",
    "    def test_on_dask_cudf(self, dask_cudf):\n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures, cols = c.sync(_get_mg_info, X)\n",
    "        \n",
    "        # Combine dask_cudf partitions with their local \"world\" instance. \n",
    "        worker_world_map = dict(map(lambda x: (x[1], x[2]), self.clique))\n",
    "        root_rank = demo.get_rank()[0]\n",
    "        \n",
    "        combined = list(map(delayed, [(worker_world_map[w], f) for w, f in gpu_futures]))\n",
    "        combined = c.compute(combined)\n",
    "        wait(combined)\n",
    "        \n",
    "        print(str(combined))\n",
    "\n",
    "        f = [c.submit(Dask_NCCL_Demo.func_test_reduce_on_partition, f, root_rank, random.random()) for f in combined]\n",
    "        wait(f)\n",
    "        \n",
    "    \n",
    "    def worker_ranks(self):\n",
    "        return dict(list(map(lambda x: (x[1], x[0]), self.clique)))\n",
    "    \n",
    "    \n",
    "    def run_func_on_workers(self, func):\n",
    "        f = [c.submit(func, a, random.random()) for i, w, a in self.clique]\n",
    "        wait(f)\n",
    "        return [a.result() for a in f]\n",
    "\n",
    "    def get_workers_(self):\n",
    "        return list(map(lambda x: Dask_NCCL_Demo.func_parse_host_port_(x), self.client.has_what().keys()))\n",
    "    \n",
    "    def init(self, uniqueId):\n",
    "        workers = self.get_workers_()\n",
    "        workers_indices = list(zip(workers, range(len(workers))))\n",
    "\n",
    "        self.clique = [(idx, worker, self.client.submit(Dask_NCCL_Demo.func_init_, \n",
    "                                           idx, \n",
    "                                           len(workers), \n",
    "                                           uniqueId,\n",
    "                                           workers=[worker]))\n",
    "             for worker, idx in workers_indices]\n",
    "        \n",
    "    def get_clique_size(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_get_size_)\n",
    "\n",
    "    def get_rank(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_get_rank_)\n",
    "    \n",
    "    def test_all_reduce(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_test_all_reduce_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe49\\xc5\\xd5d\\x7f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00_exec_result_changed-46a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x12\\x011aaf29\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x0b\\x00\\x00\\x00\\x01\\x00\\x00\\x00e\\x7f\\x00\\x00\\xa08\\xc5\\xd5d\\x7f\\x00\\x00\\xd8 \\xe7\\x05e\\x7f\\x00\\x00  0     0       0       \\x00\\x00 0\\x00\\x00}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_id = unique_id()\n",
    "world_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker process 111099 was killed by signal 6\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker process 111101 was killed by signal 6\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "demo = Dask_NCCL_Demo(c)\n",
    "demo.init(world_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.get_clique_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.worker_ranks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.test_all_reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.get_workers_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.clique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Future: status: finished, type: tuple, key: tuple-769090da-7ad2-4935-a8e6-5060fadd03a8>, <Future: status: finished, type: tuple, key: tuple-d3537952-7048-4033-a6a0-d20d91794467>]\n"
     ]
    }
   ],
   "source": [
    "X = gen_dask_cudf(2000, 50, demo)\n",
    "demo.test_on_dask_cudf(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuml4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
