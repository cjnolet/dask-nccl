{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/conda/cuml4/lib/python3.7/site-packages/distributed/bokeh/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:39400\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:35883/status' target='_blank'>http://127.0.0.1:35883/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>50.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:39400' processes=2 cores=2>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(cluster)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nccl_example import NCCL_Clique, unique_id\n",
    "import random\n",
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from helpers import _get_mg_info, gen_dask_cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dask_NCCL_Demo:\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_parse_host_port_(address):\n",
    "        if '://' in address:\n",
    "            address = address.rsplit('://', 1)[1]\n",
    "        host, port = address.split(':')\n",
    "        port = int(port)\n",
    "        return host, port\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_init_(workerId, nWorkers, uniqueId):\n",
    "        w = dask.distributed.get_worker()\n",
    "        \n",
    "        print(\"UNIQUEID: \"  + str(uniqueId))\n",
    "        \n",
    "        print(\"Hello World! from ip=%s worker=%s/%d\" % \\\n",
    "              (w.address, w.name, nWorkers))\n",
    "        a = NCCL_Clique(workerId, nWorkers)\n",
    "        a.create_clique(uniqueId)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_size_(world, r):\n",
    "        return world.get_clique_size()\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_rank_(world, r):\n",
    "        return world.get_rank()\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_device_(world, r):\n",
    "        return world.get_device()\n",
    "\n",
    "    @staticmethod\n",
    "    def func_test_all_reduce_(world, r):\n",
    "        return world.test_all_reduce()\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_test_reduce_on_partition(world_df, root_rank, r):\n",
    "        world, df = world_df\n",
    "        world.test_on_partition(df, root_rank)\n",
    "        \n",
    "        \n",
    "    def test_on_dask_cudf(self, dask_cudf):\n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures, cols = c.sync(_get_mg_info, X)\n",
    "        \n",
    "        # Combine dask_cudf partitions with their local \"world\" instance. \n",
    "        worker_world_map = dict(map(lambda x: (x[1], x[2]), self.clique))\n",
    "        root_rank = demo.get_rank()[0]\n",
    "        \n",
    "        combined = list(map(delayed, [(worker_world_map[w], f) for w, f in gpu_futures]))\n",
    "        combined = c.compute(combined)\n",
    "        wait(combined)\n",
    "        \n",
    "        print(str(combined))\n",
    "\n",
    "        f = [c.submit(Dask_NCCL_Demo.func_test_reduce_on_partition, f, root_rank, random.random()) for f in combined]\n",
    "        wait(f)\n",
    "        \n",
    "    \n",
    "    def worker_ranks(self):\n",
    "        return dict(list(map(lambda x: (x[1], x[0]), self.clique)))\n",
    "    \n",
    "    \n",
    "    def run_func_on_workers(self, func):\n",
    "        f = [c.submit(func, a, random.random()) for i, w, a in self.clique]\n",
    "        wait(f)\n",
    "        return [a.result() for a in f]\n",
    "\n",
    "    def get_workers_(self):\n",
    "        return list(map(lambda x: Dask_NCCL_Demo.func_parse_host_port_(x), self.client.has_what().keys()))\n",
    "    \n",
    "    def init(self, uniqueId):\n",
    "        workers = self.get_workers_()\n",
    "        workers_indices = list(zip(workers, range(len(workers))))\n",
    "\n",
    "        self.clique = [(idx, worker, self.client.submit(Dask_NCCL_Demo.func_init_, \n",
    "                                           idx, \n",
    "                                           len(workers), \n",
    "                                           uniqueId,\n",
    "                                           workers=[worker]))\n",
    "             for worker, idx in workers_indices]\n",
    "        \n",
    "    def get_clique_size(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_get_size_)\n",
    "\n",
    "    def get_rank(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_get_rank_)\n",
    "    \n",
    "    def test_all_reduce(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_test_all_reduce_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First demonstration will show that NCCL works within Dask as expected. This includes:\n",
    "1. Creation of a NCCL clique using Dask workers to broadcast the ncclUniqueId\n",
    "2. Demonstrating the NCCL clique successfully performs collective comms\n",
    "3. All calls to the underlying NCCL comm are made through the cuML comms facade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x02\\x00\\xd9\\x81\\xc0\\xa8\\x01\\xcf\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00%\\x7f\\x00\\x00\\xc0Wi\\xcb\\xdaU\\x00\\x00p\\x80\\x11\\xcd\\xdaU\\x00\\x00>\\xe2\\xe8\\x1a%\\x7f\\x00\\x00`w\\x15#%\\x7f\\x00\\x000a\\xff\\xe2\\xdaU\\x00\\x00\\xcd\\xbf\\xba\\x12\\xcc\\xb6\\xf7\\xc8\\x8a\\xd4\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xa7\\xd1\\xec#\\x7f\\x00\\x00\\x908\\x1b\\xec$\\x7f\\x00\\x00\\xaca\\xe1\"%\\x7f\\x00\\x00x\\xb6f\\xf0#\\x7f\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_id = unique_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = Dask_NCCL_Demo(c)\n",
    "demo.init(world_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.get_clique_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('127.0.0.1', 36376): 0, ('127.0.0.1', 43562): 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.worker_ranks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.test_all_reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('127.0.0.1', 36376), ('127.0.0.1', 43562)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.get_workers_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second demonstration will show that we can take a Dask cuDF and utilize NCCL on the workers hosting its partitions. This includes:\n",
    "1. Initializing a NCCL clique with the workers that host partitions underlying the Dask cuDF\n",
    "2. Perform a collective comm operation across the partitions using only the cuML comms facade to communicate with the NCCL comm. \n",
    "3. Output a Dask cuDF with the results of the collective comm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Future: status: finished, type: tuple, key: tuple-5c283559-d1c9-416a-9c91-032aa81c49d6>, <Future: status: finished, type: tuple, key: tuple-859463ca-47ed-4225-a6b7-374b343a7e61>]\n"
     ]
    }
   ],
   "source": [
    "X = gen_dask_cudf(2000, 50, demo)\n",
    "demo.test_on_dask_cudf(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuml4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
