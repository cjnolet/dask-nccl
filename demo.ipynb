{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an existing Dask cluster running already, set the scheduler address below. Otherwise, leave it to `None` and a local cluster will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:43591\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>50.39 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:43591' processes=2 cores=2>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler_address = None #\"tcp://10.2.168.161:8786\"\n",
    "\n",
    "if scheduler_address is None:\n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    cluster = LocalCUDACluster()\n",
    "    c = Client(cluster)\n",
    "else:\n",
    "    c = Client(scheduler_address)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nccl_example import nccl, SimpleReduce\n",
    "import random\n",
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "import numba.cuda\n",
    "import cudf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tornado import gen\n",
    "from dask.distributed import default_client\n",
    "from toolz import first\n",
    "import logging\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "import cudf\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import wait\n",
    "\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "import dask_cudf\n",
    "\n",
    "\n",
    "@gen.coroutine\n",
    "def extract_ddf_partitions(ddf):\n",
    "    \"\"\"\n",
    "    Given a Dask cuDF, return a tuple with (worker, future) for each partition\n",
    "    \"\"\"\n",
    "\n",
    "    client = default_client()\n",
    "    \n",
    "    delayed_ddf = ddf.to_delayed()\n",
    "    parts = client.compute(delayed_ddf)\n",
    "    yield wait(parts)\n",
    "    \n",
    "    key_to_part_dict = dict([(str(part.key), part) for part in parts])\n",
    "    who_has = yield client.who_has(parts)\n",
    "\n",
    "    worker_map = []\n",
    "    for key, workers in who_has.items():\n",
    "        worker = parse_host_port(first(workers))\n",
    "        worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "    gpu_data = [(worker, part) for worker, part in worker_map]\n",
    "\n",
    "    yield wait(gpu_data)\n",
    "\n",
    "    raise gen.Return(gpu_data)\n",
    "    \n",
    "    \n",
    "def create_df(f, m, n):\n",
    "    \"\"\"\n",
    "    Generates a cudf of the given size with all values initialized to 1 \n",
    "    \"\"\"\n",
    "    X = np.ones((m, n), dtype = np.float32)\n",
    "    ret = cudf.DataFrame([(i,\n",
    "                           X[:, i].astype(np.float32)) for i in range(n)],\n",
    "                         index=cudf.dataframe.RangeIndex(f * m,\n",
    "                                                         f * m + m, 1))\n",
    "    return ret\n",
    "\n",
    "def get_meta(df):\n",
    "    ret = df.iloc[:0]\n",
    "    return ret\n",
    "\n",
    "def gen_dask_cudf(nrows, ncols):\n",
    "    workers = c.has_what().keys()\n",
    "\n",
    "    # Create dfs on each worker (gpu)\n",
    "    dfs = [c.submit(create_df, n, nrows, ncols, workers=[worker])\n",
    "           for worker, n in list(zip(workers, list(range(len(workers)))))]\n",
    "    # Wait for completion\n",
    "    wait(dfs)\n",
    "\n",
    "    meta = c.submit(get_meta, dfs[0]).result()\n",
    "    return dask_cudf.from_delayed(dfs, meta=meta)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dask_NCCL_Demo:\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def func_init_(workerId, nWorkers, uniqueId):\n",
    "        w = dask.distributed.get_worker()\n",
    "\n",
    "        n = nccl()\n",
    "        n.init(nWorkers, uniqueId, workerId)\n",
    "\n",
    "        a = SimpleReduce(workerId, nWorkers, n.get_comm())\n",
    "\n",
    "        return (n, a)\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_size_(cuml_comm, r):\n",
    "        return cuml_comm[1].get_clique_size()\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_rank_(cuml_comm, r):\n",
    "        return cuml_comm[1].get_rank()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_meta(df):\n",
    "        return df.iloc[:0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_fit_transform(model, df, root_rank, r):\n",
    "        \"\"\"\n",
    "        This function is executed on the workers and performs the necessary\n",
    "        data preparation, as well as calling the cython-wrapped C++ \"algorithm\"\n",
    "        function(s), returning a cuDF with results, if necessary. \n",
    "        \n",
    "        The client will construct a Dask cuDF out of the cuDFs returned from\n",
    "        this function, if necessary. \n",
    "        \"\"\"\n",
    "\n",
    "        nccl_comm, model = model\n",
    "        \n",
    "        # Execute our cython-wrapped C++ \"algorithm\"\n",
    "        return model.fit(df).transform(df)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        An end to end example to mimic a typical Dask cuML algorithm using\n",
    "        OPG semantics. This function is executed on the client.\n",
    "        \n",
    "        The following steps are taken with Dask cuDF as input:\n",
    "        1. Co-locate Dask cuDF partitions with nccl communicator and our demo \"model\" on each worker\n",
    "        2. Run the algorithm, extracting the Numba device memory pointer for each partition\n",
    "           and allocating necessary output memory on device for constructing the cuDF \n",
    "           partition(s) that will be returned to the user. \n",
    "        3. Construct Dask cuDF from the futures containing the cuDFs returned from local \n",
    "           \"algorithm\" functions on each worker. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Keep the futures around so the GPU memory doesn't get\n",
    "        # deallocated on the workers.\n",
    "        gpu_futures = c.sync(extract_ddf_partitions, X)\n",
    "\n",
    "        worker_model_map = dict(map(lambda x: (x[1], x[2]), self.clique))\n",
    "\n",
    "        # Run our \"algorithm\" to perform reduce \n",
    "        f = [c.submit(Dask_NCCL_Demo.func_fit_transform, # Function to run on worker\n",
    "                      worker_model_map[w],               # tuple(nccl_comm, SimpleReduce instance)\n",
    "                      f,                                 # Input DataFrame partition\n",
    "                      0,                                 # Root rank\n",
    "                      random.random())                   # Makes sure all workers call function \n",
    "             for w, f in gpu_futures] \n",
    "        wait(f)\n",
    "        \n",
    "        # Convert result back into a dask_cudf\n",
    "        dfs = [d for d in f if d.type != type(None)]\n",
    "        meta = c.submit(Dask_NCCL_Demo.get_meta, dfs[0]).result()\n",
    "        ddf = dd.from_delayed(dfs, meta=meta)\n",
    "        \n",
    "        return ddf\n",
    "    \n",
    "    def worker_ranks(self):\n",
    "        return dict(list(map(lambda x: (x[1], x[0]), self.clique)))\n",
    "\n",
    "    def run_func_on_workers(self, func):\n",
    "        f = [c.submit(func, a, random.random()) for i, w, a in self.clique]\n",
    "        wait(f)\n",
    "        return [a.result() for a in f]\n",
    "\n",
    "    def get_workers_(self):\n",
    "        return list(map(lambda x: parse_host_port(x), self.client.has_what().keys()))\n",
    "    \n",
    "    def init(self, uniqueId):\n",
    "        workers = self.get_workers_()\n",
    "        workers_indices = list(zip(workers, range(len(workers))))\n",
    "\n",
    "        self.clique = [(idx, worker, self.client.submit(Dask_NCCL_Demo.func_init_, \n",
    "                                           idx, \n",
    "                                           len(workers), \n",
    "                                           uniqueId,\n",
    "                                           workers=[worker]))\n",
    "             for worker, idx in workers_indices]\n",
    "        \n",
    "    def get_clique_size(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_get_size_)\n",
    "\n",
    "    def get_rank(self):\n",
    "        return self.run_func_on_workers(Dask_NCCL_Demo.func_get_rank_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First demonstration will show that NCCL works within Dask as expected. This includes:\n",
    "1. Creation of a NCCL clique using Dask workers to broadcast the ncclUniqueId\n",
    "2. Demonstrating the NCCL clique successfully performs collective comms\n",
    "3. All calls to the underlying NCCL comm are made through the cuML comms facade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_id = nccl.get_unique_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = Dask_NCCL_Demo(c)\n",
    "demo.init(world_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.get_clique_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('127.0.0.1', 38474): 0, ('127.0.0.1', 41858): 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.worker_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second demonstration will show that we can take a Dask cuDF and utilize NCCL on the workers hosting its partitions. This includes:\n",
    "1. Initializing a NCCL clique with the workers that host partitions underlying the Dask cuDF\n",
    "2. Perform a collective comm operation across the partitions using only the cuML comms facade to communicate with the NCCL comm. \n",
    "3. Output a Dask cuDF with the results of the collective comm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gen_dask_cudf(10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:38474': ('create_df-07043ab3660c6773b23882e8e82324a2',\n",
       "  'func_init_-f21cb8486a964050e9d58ecd0cf15cb8'),\n",
       " 'tcp://127.0.0.1:41858': ('func_init_-daa712fa9e819dc0bf58ffc9d6cf6362',\n",
       "  'create_df-33511afd0dc069fa33a36235205d51ec')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.has_what()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = demo.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dask_cudf.DataFrame | 2 tasks | 1 npartitions>\n"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6 ...   49\n",
      "0  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "1  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "2  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "3  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "4  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "5  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "6  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "7  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "8  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "9  2.0  2.0  2.0  2.0  2.0  2.0  2.0 ...  2.0\n",
      "[42 more columns]\n"
     ]
    }
   ],
   "source": [
    "print(str(result.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuml4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
