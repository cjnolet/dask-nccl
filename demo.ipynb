{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an existing Dask cluster running already, set the scheduler address below. Otherwise, leave it to `None` and a local cluster will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/cjnolet/cuml5/lib/python3.7/site-packages/distributed/bokeh/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33651\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:38964/status' target='_blank'>http://127.0.0.1:38964/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>540.95 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:33651' processes=2 cores=2>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler_address = None #\"tcp://10.2.168.161:8786\"\n",
    "\n",
    "if scheduler_address is None:\n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    cluster = LocalCUDACluster(n_workers = 2)\n",
    "    c = Client(cluster)\n",
    "else:\n",
    "    c = Client(scheduler_address)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nccl_example import nccl, inject_comms_on_handle, SimpleReduce\n",
    "from cuml.common.handle import Handle\n",
    "\n",
    "import random\n",
    "from dask.distributed import wait\n",
    "\n",
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import wait\n",
    "from dask.distributed import get_worker\n",
    "\n",
    "import numba.cuda\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import asyncio\n",
    "import ucp\n",
    "\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "import numba.cuda\n",
    "import cudf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tornado import gen\n",
    "from dask.distributed import default_client\n",
    "from toolz import first\n",
    "import logging\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "import cudf\n",
    "import pandas as pd\n",
    "\n",
    "from dask.distributed import wait\n",
    "\n",
    "\n",
    "def parse_host_port(address):\n",
    "    if '://' in address:\n",
    "        address = address.rsplit('://', 1)[1]\n",
    "    host, port = address.split(':')\n",
    "    port = int(port)\n",
    "    return host, port\n",
    "\n",
    "import dask_cudf\n",
    "\n",
    "\n",
    "@gen.coroutine\n",
    "def extract_ddf_partitions(ddf):\n",
    "    \"\"\"\n",
    "    Given a Dask cuDF, return a tuple with (worker, future) for each partition\n",
    "    \"\"\"\n",
    "    client = default_client()\n",
    "    \n",
    "    delayed_ddf = ddf.to_delayed()\n",
    "    parts = client.compute(delayed_ddf)\n",
    "    yield wait(parts)\n",
    "    \n",
    "    key_to_part_dict = dict([(str(part.key), part) for part in parts])\n",
    "    who_has = yield client.who_has(parts)\n",
    "\n",
    "    worker_map = []\n",
    "    for key, workers in who_has.items():\n",
    "        worker = parse_host_port(first(workers))\n",
    "        worker_map.append((worker, key_to_part_dict[key]))\n",
    "\n",
    "    gpu_data = [(worker, part) for worker, part in worker_map]\n",
    "\n",
    "    yield wait(gpu_data)\n",
    "\n",
    "    raise gen.Return(gpu_data)\n",
    "    \n",
    "    \n",
    "def create_df(f, m, n, c):\n",
    "    \"\"\"\n",
    "    Generates a cudf of the given size with sklearn's make_blobs \n",
    "    \"\"\"\n",
    "    from sklearn.datasets.samples_generator import make_blobs\n",
    "    X, y = make_blobs(n_samples=m, centers=c, n_features=n, random_state=0)\n",
    "    ret = cudf.DataFrame([(i,\n",
    "                           X[:, i].astype(np.float64)) for i in range(n)],\n",
    "                         index=cudf.dataframe.RangeIndex(f * m,\n",
    "                                                         f * m + m, 1))\n",
    "    return ret\n",
    "\n",
    "def get_meta(df):\n",
    "    ret = df.iloc[:0]\n",
    "    return ret\n",
    "\n",
    "def gen_dask_cudf(nrows, ncols, clusters):\n",
    "    workers = c.has_what().keys()\n",
    "\n",
    "    # Create dfs on each worker (gpu)\n",
    "    dfs = [c.submit(create_df, n, nrows, ncols, clusters, workers=[worker])\n",
    "           for worker, n in list(zip(workers, list(range(len(workers)))))]\n",
    "    # Wait for completion\n",
    "    wait(dfs)\n",
    "\n",
    "    meta = c.submit(get_meta, dfs[0]).result()\n",
    "    return dask_cudf.from_delayed(dfs, meta=meta)\n",
    "\n",
    "def to_dask_cudf(futures):\n",
    "    # Convert a list of futures containing dfs back into a dask_cudf\n",
    "    dfs = [d for d in futures if d.type != type(None)]\n",
    "    meta = c.submit(get_meta, dfs[0]).result()\n",
    "    return dd.from_delayed(dfs, meta=meta)\n",
    "\n",
    "\n",
    "async def connection_func(ep, listener):\n",
    "    print(\"connection received from \" + str(ep))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommsBase:\n",
    "    \n",
    "    def __init__(self, comms_coll = True, comms_p2p = False):\n",
    "        self.client = default_client()\n",
    "        self.comms_coll = comms_coll\n",
    "        self.comms_p2p = comms_p2p\n",
    "        \n",
    "        # Used to identify this distinct session on workers\n",
    "        self.sessionId = uuid.uuid4().bytes\n",
    "            \n",
    "        self.worker_addresses = self.get_workers_()\n",
    "        self.workers = list(map(lambda x: parse_host_port(x), self.worker_addresses))\n",
    "\n",
    "    def __dealloc__(self):\n",
    "        self.destroy()\n",
    "\n",
    "    def get_workers_(self, parse_address = True):\n",
    "        \"\"\"\n",
    "        Return the list of workers parsed as [(address, port)]\n",
    "        \"\"\"\n",
    "        return list(self.client.has_what().keys())\n",
    "    \n",
    "    def worker_ranks(self):\n",
    "        \"\"\"\n",
    "        Builds a dictionary of { (worker_address, worker_port) : worker_rank }\n",
    "        \"\"\"\n",
    "        return dict(list(map(lambda x: (x[1], x[0]), self.nccl_clique)))\n",
    "    \n",
    "    def worker_ports(self):\n",
    "        \"\"\"\n",
    "        Builds a dictionary of { (worker_address, worker_port) : worker_port }\n",
    "        \"\"\"\n",
    "        return dict(list(self.ucp_ports))\n",
    "    \n",
    "    \n",
    "    def worker_info(self):\n",
    "        \"\"\"\n",
    "        Builds a dictionary of { (worker_address, worker_port) : (worker_rank, worker_port ) }\n",
    "        \"\"\"\n",
    "        ranks = self.worker_ranks() if self.comms_coll else None\n",
    "        ports = self.worker_ports() if self.comms_p2p else None\n",
    "        \n",
    "        if self.comms_coll and self.comms_p2p:\n",
    "            output = {}\n",
    "            for k in self.worker_ranks().keys():\n",
    "                output[k] = (ranks[k], ports[k])\n",
    "            return output\n",
    "                \n",
    "        elif self.comms_coll:\n",
    "            return ranks\n",
    "        elif self.comms_p2p:\n",
    "            return ports\n",
    "\n",
    "    @staticmethod\n",
    "    def func_init_nccl(workerId, nWorkers, uniqueId):\n",
    "        \"\"\"\n",
    "        Initialize ncclComm_t on worker\n",
    "        \"\"\"\n",
    "        n = nccl()\n",
    "        n.init(nWorkers, uniqueId, workerId)\n",
    "        return n\n",
    "\n",
    "    @staticmethod\n",
    "    def func_get_ucp_port(sessionId, r):\n",
    "        \"\"\"\n",
    "        Return the port assigned to a UCP listener on worker\n",
    "        \"\"\"\n",
    "        dask_worker = get_worker()\n",
    "        port = dask_worker.data[sessionId].port\n",
    "        return port\n",
    "\n",
    "    @staticmethod\n",
    "    async def ucp_create_listener(sessionId, r):\n",
    "        dask_worker = get_worker()\n",
    "        if sessionId in dask_worker.data:\n",
    "            print(\"Listener already started for sessionId=\" + str(sessionId))\n",
    "        else:\n",
    "            ucp.init()\n",
    "            listener =  ucp.start_listener(connection_func, 0, is_coroutine=True)\n",
    "\n",
    "            dask_worker.data[sessionId] = listener\n",
    "            task = asyncio.create_task(listener.coroutine)\n",
    "\n",
    "            while not task.done():\n",
    "                await task\n",
    "                await asyncio.sleep(1)\n",
    "                \n",
    "            ucp.fin()\n",
    "            del dask_worker.data[sessionId]\n",
    "            \n",
    "\n",
    "            \n",
    "    @staticmethod\n",
    "    def ucp_stop_listener(sessionId, r):\n",
    "        dask_worker = get_worker()\n",
    "        if sessionId in dask_worker.data:\n",
    "            listener = dask_worker.data[sessionId]\n",
    "            ucp.stop_listener(listener)\n",
    "        else:\n",
    "            print(\"Listener not found with sessionId=\" + str(sessionId))\n",
    "\n",
    "    def create_ucp_listeners(self):\n",
    "        \"\"\"\n",
    "        Build a UCP listener on each worker. Since this async function is long-running, the listener is \n",
    "        placed in the worker's data dict. \n",
    "        \n",
    "        NOTE: This is not the most ideal design because the worker's data dict could be serialized at \n",
    "        any point, which would cause an error. Need to sync w/ the Dask team to see if there's a better\n",
    "        way to do this. \n",
    "        \"\"\"\n",
    "        [self.client.run(CommsBase.ucp_create_listener, self.sessionId, random.random(), workers = [w], wait = False) for w in \n",
    "         self.worker_addresses]\n",
    "        \n",
    "    def get_ucp_ports(self):\n",
    "        \"\"\"\n",
    "        Return the UCP listener ports attached to this session\n",
    "        \"\"\"\n",
    "        self.ucp_ports = [(w, self.client.submit(CommsBase.func_get_ucp_port, self.sessionId, random.random(), workers = [w]).result()) \n",
    "                          for w in self.workers]\n",
    "        \n",
    "    def stop_ucp_listeners(self):\n",
    "        \"\"\"\n",
    "        Stops the UCP listeners attached to this session\n",
    "        \"\"\"\n",
    "        a = [c.submit(CommsBase.ucp_stop_listener, self.sessionId, random.random(), workers=[w]) \n",
    "         for w in self.workers]\n",
    "        wait(a)\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_build_handle(nccl_comm, eps, nWorkers, workerId):\n",
    "        \n",
    "        ucp_worker = ucp.get_ucp_worker()\n",
    "        \n",
    "        handle = Handle()\n",
    "        inject_comms_on_handle(handle, nccl_comm, ucp_worker, eps, nWorkers, workerId)\n",
    "        return handle\n",
    "\n",
    "    def init_nccl(self):\n",
    "        \"\"\"\n",
    "        Use nccl-py to initialize ncclComm_t on each worker and \n",
    "        store the futures for this instance. \n",
    "        \"\"\"\n",
    "        self.uniqueId = nccl.get_unique_id()\n",
    "\n",
    "        workers_indices = list(zip(self.workers, range(len(self.workers))))\n",
    "\n",
    "        self.nccl_clique = [(idx, worker, self.client.submit(CommsBase.func_init_nccl, \n",
    "                                           idx, \n",
    "                                           len(self.workers), \n",
    "                                           self.uniqueId,\n",
    "                                           workers=[worker]))\n",
    "             for worker, idx in workers_indices]\n",
    "        \n",
    "    def init_ucp(self):\n",
    "        \"\"\"\n",
    "        Use ucx-py to initialize ucp endpoints so that every \n",
    "        worker can communicate, point-to-point, with every other worker\n",
    "        \"\"\"\n",
    "        self.create_ucp_listeners()\n",
    "        self.get_ucp_ports()\n",
    "        self.ucp_create_endpoints()\n",
    "        \n",
    "    def init(self):\n",
    "        if self.comms_coll:\n",
    "            self.init_nccl()\n",
    "        \n",
    "        if self.comms_p2p:\n",
    "            self.init_ucp()\n",
    "            \n",
    "        # Combine ucp ports w/ nccl ranks\n",
    "            \n",
    "        eps_futures = dict(self.ucp_endpoints)\n",
    "            \n",
    "        self.handles = [(wid, w, self.client.submit(CommsBase.func_build_handle, f, eps_futures[w], len(self.workers), wid, workers = [w])) \n",
    "                        for wid, w, f in self.nccl_clique]\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    async def func_ucp_create_endpoints(sessionId, worker_info, r):\n",
    "        \"\"\"\n",
    "        Runs on each worker to create ucp endpoints to all other workers\n",
    "        \"\"\"\n",
    "        dask_worker = get_worker()\n",
    "        local_address = parse_host_port(dask_worker.address)\n",
    "        \n",
    "        eps = [None]*len(worker_info)\n",
    "        \n",
    "        count = 1\n",
    "        size = len(worker_info)-1\n",
    "        \n",
    "        for k in worker_info:\n",
    "            if k != local_address:\n",
    "                ip, port = k\n",
    "                rank, ucp_port = worker_info[k]\n",
    "                ep = await ucp.get_endpoint(ip.encode(), ucp_port, timeout = 1)\n",
    "                eps[rank] = ep\n",
    "                count +=1\n",
    "        dask_worker.data[str(sessionId) + \"_eps\"] = eps\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_get_endpoints(sessionId, r):\n",
    "        \"\"\"\n",
    "        Fetches (and removes) the endpoints from the worker's data dict\n",
    "        \"\"\"\n",
    "        dask_worker = get_worker()\n",
    "        eps = dask_worker.data[str(sessionId)+\"_eps\"]\n",
    "        del dask_worker.data[str(sessionId)+\"_eps\"]\n",
    "        return eps\n",
    "        \n",
    "    def ucp_create_endpoints(self):\n",
    "        \n",
    "        worker_info = self.worker_info()\n",
    "        \n",
    "        [self.client.run(CommsBase.func_ucp_create_endpoints, self.sessionId, worker_info, random.random(), workers = [w], wait = True)\n",
    "                          for w in self.worker_addresses]\n",
    "        \n",
    "        ret = [(w, self.client.submit(CommsBase.func_get_endpoints, self.sessionId, random.random(), workers=[w])) for w in self.workers]\n",
    "        wait(ret)\n",
    "        \n",
    "        self.ucp_endpoints = ret\n",
    "\n",
    "    @staticmethod\n",
    "    def func_destroy_nccl(nccl_comm, r):\n",
    "        \"\"\"\n",
    "        Destroys NCCL communicator on worker\n",
    "        \"\"\"\n",
    "        nccl_comm.destroy()\n",
    "        \n",
    "\n",
    "    def destroy_nccl(self):\n",
    "        \"\"\"\n",
    "        Destroys all NCCL communicators on workers\n",
    "        \"\"\"\n",
    "        a = [self.client.submit(CommsBase.func_destroy_nccl, f, random.random(), workers=[w]) for wid, w, f in self.nccl_clique]\n",
    "        wait(a)\n",
    "        \n",
    "    def func_destroy_ep(eps, r):\n",
    "        \"\"\"\n",
    "        Destroys UCP endpoints on worker\n",
    "        \"\"\"\n",
    "        for ep in eps:\n",
    "            if ep is not None:\n",
    "                ucp.destroy_ep(ep)\n",
    "                \n",
    "    def destroy_eps(self):\n",
    "        \"\"\"\n",
    "        Destroys all UCP endpoints on all workers\n",
    "        \"\"\"\n",
    "        a = [self.client.submit(CommsBase.func_destroy_ep, f, random.random(), workers = [w]) for w, f in self.ucp_endpoints]\n",
    "        wait(a)\n",
    "        \n",
    "    def destroy_ucp(self):\n",
    "        self.destroy_eps()\n",
    "        self.stop_ucp_listeners()\n",
    "        \n",
    "\n",
    "    def destroy(self):\n",
    "\n",
    "        self.handles = None\n",
    "        \n",
    "        if self.comms_p2p:\n",
    "            self.destroy_ucp()\n",
    "            self.ucp_ports = None\n",
    "            self.ucp_endpoints = None\n",
    "\n",
    "        if self.comms_coll:\n",
    "            # TODO: Figure out why this fails when UCP + NCCL are both used\n",
    "#             self.destroy_nccl()\n",
    "            self.nccl_clique = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dask_NCCL_Demo(CommsBase):\n",
    "    \n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        \n",
    "        super(Dask_NCCL_Demo, self).__init__(comms_coll = True, comms_p2p = True)\n",
    "        self.init_()\n",
    "\n",
    "    def init_(self):\n",
    "        \"\"\"\n",
    "        Creates local kmeans instance on each worker\n",
    "        \"\"\"\n",
    "        self.init()\n",
    "        \n",
    "        self.models = [(w, c.submit(Dask_NCCL_Demo.func_build_model_, \n",
    "                                    i, len(self.workers), a,\n",
    "                                    workers=[w])) for i, w, a in self.handles]\n",
    "        wait(self.models)\n",
    "\n",
    "    @staticmethod\n",
    "    def func_build_model_(wid, n_workers, handle):\n",
    "        \"\"\"\n",
    "        Create local KMeans instance on worker\n",
    "        \"\"\"\n",
    "        w = dask.distributed.get_worker()\n",
    "        \n",
    "        return SimpleReduce(wid, n_workers, handle)\n",
    "    \n",
    "    @staticmethod\n",
    "    def func_fit(model, df, wid): return model.fit(df)\n",
    "    \n",
    "    def run_model_func_on_dask_cudf(self, func, X):\n",
    "        \n",
    "        gpu_futures = c.sync(extract_ddf_partitions, X)\n",
    "\n",
    "        worker_model_map = dict(map(lambda x: (x[0], x[1]), self.models))\n",
    "        worker_rank_map = self.worker_ranks()\n",
    "\n",
    "        f = [c.submit(func,                              # Function to run on worker\n",
    "                      worker_model_map[w],               # Model instance\n",
    "                      f,                                 # Input DataFrame partition\n",
    "                      worker_rank_map[w])                # Worker ID\n",
    "             for w, f in gpu_futures]\n",
    "        wait(f)\n",
    "        return f\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.run_model_func_on_dask_cudf(Dask_NCCL_Demo.func_fit, X)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First demonstration will show that NCCL works within Dask as expected. This includes:\n",
    "1. Creation of a NCCL clique using Dask workers to broadcast the ncclUniqueId\n",
    "2. Demonstrating the NCCL clique successfully performs collective comms\n",
    "3. All calls to the underlying NCCL comm are made through the cuML comms facade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = Dask_NCCL_Demo(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_worker = 10\n",
    "n_features = 50\n",
    "n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gen_dask_cudf(n_samples_per_worker, n_features, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dask_NCCL_Demo at 0x7f2a7803e3c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('127.0.0.1', 38277): 0, ('127.0.0.1', 39992): 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.worker_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second demonstration will show that we can take a Dask cuDF and utilize NCCL on the workers hosting its partitions. This includes:\n",
    "1. Initializing a NCCL clique with the workers that host partitions underlying the Dask cuDF\n",
    "2. Perform a collective comm operation across the partitions using only the cuML comms facade to communicate with the NCCL comm. \n",
    "3. Output a Dask cuDF with the results of the collective comm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gen_dask_cudf(10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:32861': ('func_init_-a71c233aee224fc26dbb5814fbb430b5',\n",
       "  'create_df-a197a3749394220cdb46b22d7e22e5aa'),\n",
       " 'tcp://127.0.0.1:36206': ('create_df-b486209d0d3d8d405fd3f412af4d93dd',\n",
       "  'func_init_-4c59850e03537d05ee83232f7ea71c4d'),\n",
       " 'tcp://127.0.0.1:36896': ('create_df-a6c56a9a8a51cf154cee198e492c9d87',\n",
       "  'func_init_-67a05abe09e32437147498edef0baf67'),\n",
       " 'tcp://127.0.0.1:38656': ('func_init_-9816649ccd06b4956e11d95970db14c1',\n",
       "  'create_df-26043e82cb25e74bf26ec2a3f625642f'),\n",
       " 'tcp://127.0.0.1:39491': ('create_df-2df214e766a1dbf9480e3fbeddb84ac6',\n",
       "  'func_init_-96945057e95fe70f778bcb12808f299a'),\n",
       " 'tcp://127.0.0.1:41502': ('func_init_-6ba15b369e688198c7fd32037738c942',\n",
       "  'create_df-731fa1c943ef87ae06019d6b89065f7a'),\n",
       " 'tcp://127.0.0.1:45176': ('create_df-3525375ac9aeb05d87b868388a97edd4',\n",
       "  'func_init_-a55726ee6c1f5fddf0be5108813d4a22'),\n",
       " 'tcp://127.0.0.1:45850': ('func_init_-7683987ccfbd6e8c534b625ded2bc5df',\n",
       "  'create_df-cebb128559a98cf04b31d750867db360')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.has_what()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = demo.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dask_cudf.DataFrame | 2 tasks | 1 npartitions>\n"
     ]
    }
   ],
   "source": [
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6 ...   49\n",
      "0  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "1  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "2  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "3  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "4  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "5  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "6  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "7  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "8  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "9  8.0  8.0  8.0  8.0  8.0  8.0  8.0 ...  8.0\n",
      "[42 more columns]\n"
     ]
    }
   ],
   "source": [
    "print(str(result.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuml5 [conda env:cuml5]",
   "language": "python",
   "name": "conda-env-cuml5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
